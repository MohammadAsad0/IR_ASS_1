# -*- coding: utf-8 -*-
"""IR-Boolean Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nQv-qSjDI0FaZ1Cnr4XUZMH5A42v0UqM
"""

# importing libraries
import os
import re

import nltk
from nltk.stem import PorterStemmer

import json
import pickle


# to read the stopword-list.txt and store them in stopwrds variable
def ReadStopWordFile(path):
  with open(path, "r") as stopword_file:
    stop_content = stopword_file.read()

    stop_content = stop_content.lower()

    stopwords = stop_content.split()

    return stopwords

# to read all the documents and store them in fileContents variable
def ReadDataset(path):
  for fileName in os.listdir(path):
    
    if fileName.endswith(".txt"):

      pure_filename = os.path.splitext(fileName)[0]
      
      with open(os.path.join(path, fileName), "r") as file:
        
        content = file.read();
        fileContents[pure_filename] = content


# create term document matrix
def CreateTermDocMatrix(stopwords):
  # term doc matrix to store all words and their doc id
  term_doc_mat = []

  for index, (key, item) in enumerate(fileContents.items()):
    # lower casing each document
    item = item.lower()

    # removing given stopwords from each document
    for i in stopwords:
      if i in str(item):
        item = re.sub(str(' ' + i + ' '), ' ', str(item))

    # splitting each document text into separate words
    words = item.split()

    for i, word in enumerate(words):
      # cleans words
      word = re.sub(r'[^\w\s]','', word)

      if(word != ''):
        # stemming each word
        stem_word = stemmer.stem(word)

        term_doc_mat.append([stem_word, key])

  # sorting
  term_doc_mat = sorted(term_doc_mat)
  
  return term_doc_mat
  

# creates inverted index from term doc matrix
def CreateInvertedIndex(term_doc_mat):
  for item in term_doc_mat:
    term = item[0]
    docId = int(item[1])

    if term not in dictionary:
      inverted_index[term] = [1, [docId]]
      dictionary.append(term)

    else:
      inverted_index[term][0] = inverted_index[term][0] + 1

      if inverted_index[term][1][-1] != docId:
        inverted_index[term][1].append(docId)


# search for boolean queries
def search(query):
  result = []
  operators = []

  query = query.lower()

  query_terms = query.split()
  query_count = 0

  final_result = []

  for index, term in enumerate(query_terms):

    if index % 2 == 0:
      query_count += 1
      stem_term = stemmer.stem(term)
      
      try:
        result.append(inverted_index[stem_term][1])
      except:
        result.append([])
    else:
      operators.append(term)


  result_count = len(result)

  final_result = result[0]
  
  if result_count > 1:
    for index, item in enumerate(operators):
      try:
        if item == "and":
          final_result = set(final_result).intersection(set(result[index+1]))
        
        elif item == "or":
          final_result = set(final_result).union(set(result[index+1]))

        elif item == "not":
          final_result = set(final_result) - set(result[index+1])
      
      except Exception as e:
        print(e)

  return sorted(list(final_result))


"""Positional Index"""

def CreatePositionalTermDocMatrix():
  # term doc matrix to store all words, their doc id, and their postions
  pos_term_doc_mat = []

  for index, (key, item) in enumerate(fileContents.items()):
    # lower casing each document
    item = item.lower()

    # for storing positions
    all_words = item.split()

    for index, word in enumerate(all_words):
      # cleans words
      word = re.sub(r'[^\w\s]','', word)

      # removing given stopwords from each word
      # for i in stopwords:
      #   if i == str(word):
      #     word = re.sub(i, '', str(word))

      if(word != ''):
        stem_word = stemmer.stem(word)
        pos_term_doc_mat.append([stem_word, key, index])

  # sorting
  pos_term_doc_mat = sorted(pos_term_doc_mat)
  
  return pos_term_doc_mat
  

def CreatePositionalInvertedIndex(pos_term_doc_mat):
  for item in pos_term_doc_mat:
    term = item[0]
    docId = int(item[1])
    pos = int(item[2])

    if term not in pos_dictionary:
      # new dictionary and inverted index item
      pos_inverted_index[term] = [1, {docId: []}]

      pos_dictionary.append(term)

    else:
      # incrementing frequency
      pos_inverted_index[term][0] = pos_inverted_index[term][0] + 1

      # checking if docId exists. If not then creating new dictionary => {docId: []}
      if pos_inverted_index[term][1].get(docId) == None:
        pos_inverted_index[term][1][docId] = []

    # appending to the array of positions inside dictionary of docId 
    pos_inverted_index[term][1][docId].append(pos)


def PhraseSearch(query):
  # lower case
  query = query.lower()

  # splitting into list of words
  query_terms = query.split()

  # it will contain resulting inverted index for each query term
  result = []

  # filling result variable
  for q_term in query_terms:
    
    stem_term = stemmer.stem(q_term)

    try:
      result.append(pos_inverted_index[stem_term][1])
    except:
      result.append({})

  # this will contain common doc ids for each query term
  common_docs = result[0].keys()

  # taking intersection to get common documents
  for inv_index in result[1:]:
    common_docs = set(common_docs).intersection(set(inv_index.keys()))

  # it will contain final resulting doc ids
  final_result = []

  for doc in common_docs:
    lst = result[0][doc]
    i = 1

    for query_i in range(1, len(result)):

      temp_lst = [x - i for x in result[query_i][doc]]

      lst = set(lst).intersection(set(temp_lst))

      i += 1
    
    if lst:
      final_result.append(doc)

  return sorted(final_result)


def ProximitySearch(query):
  # lower case
  query = query.lower()

  # splitting into list of words
  query_terms = query.split()

  # it will contain resulting inverted index for each query term
  result = []
  k_set = []

  # filling result variable
  for index, q_term in enumerate(query_terms):
    
    if index % 2 == 0:
      stem_term = stemmer.stem(q_term)

      try:
        result.append(pos_inverted_index[stem_term][1])
      except:
        result.append({})
    
    else:
      try:
        k_set.append(int(q_term[1:]))
      except:
        return []
      
  # this will contain common doc ids for each query term
  common_docs = result[0].keys()

  # taking intersection to get common documents
  for inv_index in result[1:]:
    common_docs = set(common_docs).intersection(set(inv_index.keys()))

  # it will contain final resulting doc ids
  final_result = []

  for doc in common_docs:
    lst = result[0][doc]
    i = 0

    for query_i in range(1, len(result)):

      temp_lst = [x - k_set[i] for x in result[query_i][doc]]

      lst = set(lst).intersection(set(temp_lst))

      i += 1
    
    if lst:
      final_result.append(doc)

  return sorted(final_result)
      



def StoreIndexInFile(path, index):
  if index:
    byte_stream = pickle.dumps(index)
    
    with open(path, "wb") as file:
      file.write(byte_stream)


def ReadIndexFromFile(path):
  with open(path, "rb") as file:
    byte_stream = file.read()
    inv_index = pickle.loads(byte_stream)
    
    return inv_index


def PrepareFileContents():
  # datastet path
  path = "./Dataset/"

  # stopwords path
  stopword_file_path = "./Stopword-List.txt"

  stopwords = ReadStopWordFile(stopword_file_path)

  ReadDataset(path)
  
  return stopwords


# for stemming
stemmer = PorterStemmer()

# inverted Index in the form of -> term: [freq, [docID...]]
inverted_index = {}

# inverted Index in the form of -> term: [freq, [{docId: [pos, ...]}, ...]]
pos_inverted_index = {}

fileContents = {}

inv_index_path = "InvertedIndex.pickle"
pos_index_path = "PositionalIndex.pickle"


if os.path.exists(inv_index_path):
  inverted_index = ReadIndexFromFile(inv_index_path)

else:
  stopwords = PrepareFileContents()
  
  term_doc_matrix = CreateTermDocMatrix(stopwords)

  # Dictionary to store unique terms
  dictionary = []

  CreateInvertedIndex(term_doc_matrix)
  
  StoreIndexInFile(inv_index_path, inverted_index)
  

if os.path.exists(pos_index_path):
  pos_inverted_index = ReadIndexFromFile(pos_index_path)
  
else:
  PrepareFileContents()
  
  # [term, docId, position]
  positional_term_doc_matrix = CreatePositionalTermDocMatrix()

  # Dictionary to store unique terms
  pos_dictionary = []

  CreatePositionalInvertedIndex(positional_term_doc_matrix)
  
  StoreIndexInFile(pos_index_path, pos_inverted_index)